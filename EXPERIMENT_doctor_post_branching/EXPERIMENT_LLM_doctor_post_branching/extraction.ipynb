{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df76bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_prefix_space=True, local_files_only = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=tokenizer_with_prefix_space.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3291b3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[657], [352], [362], [513], [604], [642], [718], [767], [807], [860]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e18a2b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc324540",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \".\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output[0]\n",
    "    probs = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cd5a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "indexes.append([tokenizer.eos_token_id])\n",
    "\n",
    "word_probs = {}\n",
    "for i in indexes:\n",
    "    word_prob = torch.prod(torch.stack([probs[0, -1, idx] for idx in i]))\n",
    "    word_probs[tokenizer.decode(i)] = word_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2775fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 0': 0.03385974360664388,\n",
       " ' 1': 0.03806256264214874,\n",
       " ' 2': 0.018095353245507095,\n",
       " ' 3': 0.018888392716423955,\n",
       " ' 4': 0.022219657806359656,\n",
       " ' 5': 0.026736669754847217,\n",
       " ' 6': 0.01715849815160742,\n",
       " ' 7': 0.022544613293687843,\n",
       " ' 8': 0.014472734370450423,\n",
       " ' 9': 0.015038818295953986,\n",
       " '<|endoftext|>': 0.7729229561163697}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_word_probs = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "298d3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "\n",
    "word_probs = {}\n",
    "for i in indexes:\n",
    "    word_prob = torch.prod(torch.stack([probs[0, -1, idx] for idx in i]))\n",
    "    word_probs[tokenizer.decode(i)] = word_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "951c2f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 0': 0.1491112576927676,\n",
       " ' 1': 0.16761959725728418,\n",
       " ' 2': 0.07968816634225867,\n",
       " ' 3': 0.08318054697815978,\n",
       " ' 4': 0.0978507445153572,\n",
       " ' 5': 0.1177427242207227,\n",
       " ' 6': 0.07556245166024182,\n",
       " ' 7': 0.09928178079172653,\n",
       " ' 8': 0.06373490742581289,\n",
       " ' 9': 0.06622782311566863}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_word_probs = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3eb3da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probs(prompt, eos, numbers):\n",
    "\n",
    "    #input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)  \n",
    "    str_seq = [tokenizer.tokenize(x) for x in prompt]\n",
    "    str_seq = [item for tokens in str_seq for item in tokens]\n",
    "    prompt_ids = tokenizer.convert_tokens_to_ids(str_seq)\n",
    "    print(prompt_ids)\n",
    "    input_ids = torch.tensor(prompt_ids).reshape(1, -1) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            logits = output.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)[0] \n",
    "    indexes = [tokenizer.encode(number) for number in numbers]\n",
    "    print(list(indexes))\n",
    "    if eos:\n",
    "        indexes.append([tokenizer.eos_token_id])\n",
    "    word_probs = {}\n",
    "    for i in indexes:\n",
    "        word_prob = probs[i]\n",
    "        word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "    normalized_word_probs = {}\n",
    "    total = sum(word_probs.values())\n",
    "    for word in word_probs:\n",
    "        normalized_word_probs[word] = word_probs[word] / total\n",
    "    return normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e01fe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"He\", \"She\"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e85d1ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[383, 6253, 373, 845, 5863, 13]\n",
      "[[679], [1375]]\n",
      "{'He': 0.9199779194274089, 'She': 0.08002208057259108}\n",
      "[383, 6253, 373, 845, 5863, 13, 679]\n",
      "[[679], [1375]]\n",
      "{'He': 0.8728291624313986, 'She': 0.12717083756860142}\n",
      "The doctor was very famous. He died\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    next_token = \"\"\n",
    "    prompt = [\"The doctor was very famous.\"]\n",
    "    min_digits = 1 + (len(symbols) - 1)\n",
    "    max_digits = len(symbols) + len(symbols_dummy[i].split(\" \"))\n",
    "    count_branching = 0\n",
    "    \n",
    "    while next_token != tokenizer.eos_token:\n",
    "        if len(prompt) >= min_digits + 1:\n",
    "            break\n",
    "            normalized_word_probs = calculate_probs(prompt, True, symbols_dummy[i].split(\" \"))\n",
    "        else:\n",
    "            normalized_word_probs_he_she = calculate_probs(prompt, False, symbols)\n",
    "            print(normalized_word_probs_he_she)\n",
    "\n",
    "        if count_branching == 0:\n",
    "            next_token = np.random.choice(list(normalized_word_probs_he_she.keys()), p=list(normalized_word_probs_he_she.values()))\n",
    "            count_branching = 1\n",
    "        else:\n",
    "            next_token = np.random.choice(list(normalized_word_probs.keys()), p=list(normalized_word_probs.values()))\n",
    "        \n",
    "        if next_token != tokenizer.eos_token:\n",
    "            prompt.append(next_token)\n",
    "        \n",
    "        if len(prompt) - 1 >= max_digits:\n",
    "            next_token = tokenizer.eos_token\n",
    "\n",
    "    print(' '.join(prompt))\n",
    "    results.append(' '.join(prompt))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"floating-point\"])\n",
    "df.to_csv(\"floating_points_LLM.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b5fa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"He\", \"She\"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc6ea874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['died', 'yesterday', 'in', 'a', 'very', 'sad.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_dummy[5].split(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
