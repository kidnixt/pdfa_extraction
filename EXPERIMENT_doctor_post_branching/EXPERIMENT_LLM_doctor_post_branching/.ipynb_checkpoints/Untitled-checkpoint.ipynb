{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808e1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_prefix_space=True, local_files_only = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c440f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_important = [\"He\", \"She\"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c488969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   383,  6253,   373,   845,  5863,    13]])\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous.\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64eae69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'He': 0.9474213481974543, 'She': 0.0525786518025457}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output.logits[:, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "\n",
    "word_probs = {}\n",
    "symbols_important_ids = [tokenizer.encode(sym) for sym in symbols_important]\n",
    "\n",
    "for i in symbols_important_ids:\n",
    "    word_prob = probs[i]\n",
    "    word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "\n",
    "normalized_word_probs_he_she = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs_he_she[word] = word_probs[word] / total\n",
    "\n",
    "    \n",
    "normalized_word_probs_he_she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f677d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['died.'],\n",
       " ['died', 'yesterday.'],\n",
       " ['died', 'yesterday', 'in.'],\n",
       " ['died', 'yesterday', 'in', 'a.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', 'a.'],\n",
       " ['died',\n",
       "  'yesterday',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'sad',\n",
       "  'way',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'heart.'],\n",
       " ['died',\n",
       "  'yesterday',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'sad',\n",
       "  'way',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'heart',\n",
       "  'attack.']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lists = [sentence.split() for sentence in symbols_dummy]\n",
    "split_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fef3e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_with_prompt(split_lists, index, tokenizer, model, device, symbol_choosen):\n",
    "    sentence = split_lists[index]\n",
    "    prompt = tokenizer.decode(tokenizer.bos_token_id) + \" The doctor was very famous.\" + symbol_choosen\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        word_probs.clear()\n",
    "        current_sentence = sentence[i:]\n",
    "        symbols_dummy_ids = [tokenizer.encode(sym) for sym in current_sentence]\n",
    "        symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "        prompt += \" \" + current_sentence[0]\n",
    "        input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            logits = output.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)[0]\n",
    "\n",
    "        for token_ids in symbols_dummy_ids:\n",
    "            for token in token_ids:\n",
    "                word_prob = probs[token]\n",
    "                word_probs[tokenizer.decode(token).replace(\" \",\"\")] = word_prob.item()\n",
    "\n",
    "        # Normalización de probabilidades\n",
    "        normalized_word_probs_dummy = {}\n",
    "        total = sum(word_probs.values())\n",
    "        for word in word_probs:\n",
    "            normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "\n",
    "        #print(f\"Normalized probabilities for '{' '.join(current_sentence)}': {normalized_word_probs_dummy}\")\n",
    "    \n",
    "    return normalized_word_probs_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b0e0fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Selección aleatoria entre \"He\" y \"She\"\n",
    "symbol_choosen = np.random.choice(list(normalized_word_probs_he_she.keys()), \n",
    "                                  p=list(normalized_word_probs_he_she.values()))\n",
    "normalized_word_probs_dummy = calculate_probabilities_with_prompt(split_lists, 1, tokenizer, model, device, symbol_choosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf3e3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a147612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "def run_n_times(n, split_lists):\n",
    "    all_times = []  # Lista para almacenar los tiempos de todas las llamadas\n",
    "\n",
    "    for sentence_index, sentence in enumerate(split_lists):\n",
    "        times = []  # Lista para almacenar los tiempos de cada oración\n",
    "\n",
    "        for _ in range(n):  # Llamar a la función n veces\n",
    "            start_time = time()\n",
    "            symbol_chosen = np.random.choice(list(normalized_word_probs_he_she.keys()), \n",
    "                                              p=list(normalized_word_probs_he_she.values()))\n",
    "            \n",
    "            # Llamar a la función con el índice de la oración actual\n",
    "            normalized_word_probs_dummy = calculate_probabilities_with_prompt(split_lists, sentence_index, tokenizer, model, device, symbol_chosen)\n",
    "            elapsed_time = time() - start_time\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "        all_times.append(times)  # Agregar los tiempos de esta oración a la lista principal\n",
    "\n",
    "    return all_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842493a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> The doctor was very famous.He died.\n",
      "0.05665898323059082\n",
      "<|endoftext|> The doctor was very famous.He died.\n",
      "0.03992962837219238\n",
      "<|endoftext|> The doctor was very famous.He died.\n",
      "0.03573489189147949\n",
      "<|endoftext|> The doctor was very famous.He died.\n",
      "0.034593820571899414\n",
      "<|endoftext|> The doctor was very famous.She died.\n",
      "0.03377103805541992\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n",
      "0.06558418273925781\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n",
      "0.06629729270935059\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n",
      "0.07467532157897949\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n",
      "0.0690300464630127\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday.\n",
      "0.07265877723693848\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in.\n",
      "0.10629415512084961\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in.\n",
      "0.11088848114013672\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in.\n",
      "0.09995579719543457\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in.\n",
      "0.10285782814025879\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in.\n",
      "0.10004878044128418\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in a.\n",
      "0.13826847076416016\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in a.\n",
      "0.14528322219848633\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in a.\n",
      "0.13881492614746094\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in\n",
      "<|endoftext|> The doctor was very famous.He died yesterday in a.\n",
      "0.13706254959106445\n",
      "<|endoftext|> The doctor was very famous.He died\n",
      "<|endoftext|> The doctor was very famous.He died yesterday\n"
     ]
    }
   ],
   "source": [
    "n = 5  \n",
    "execution_times = run_n_times(n, split_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3186925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['died', '.'],\n",
       " ['died', 'yesterday', '.'],\n",
       " ['died', 'yesterday', 'in', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', '.'],\n",
       " ['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', 'a', '.'],\n",
       " ['died',\n",
       "  'yesterday',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'sad',\n",
       "  'way',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'heart',\n",
       "  '.'],\n",
       " ['died',\n",
       "  'yesterday',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'sad',\n",
       "  'way',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'heart',\n",
       "  'attack',\n",
       "  '.']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "split_lists_dot = [re.findall(r'\\S+|\\.', sentence.replace('.', ' .')) for sentence in symbols_dummy]\n",
    "split_lists_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a9ee892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_with_prompt_dot(split_lists, index, tokenizer, model, device, symbol_choosen):\n",
    "    sentence = split_lists_dot[index]\n",
    "    prompt = tokenizer.decode(tokenizer.bos_token_id) + \" The doctor was very famous.\" + symbol_choosen\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        word_probs.clear()\n",
    "        current_sentence = sentence[i:]\n",
    "        symbols_dummy_ids = [tokenizer.encode(sym) for sym in current_sentence]\n",
    "        symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "        prompt += \" \" + current_sentence[0]\n",
    "        input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "        \n",
    "        print(prompt)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            logits = output.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)[0]\n",
    "\n",
    "        for token_ids in symbols_dummy_ids:\n",
    "            for token in token_ids:\n",
    "                word_prob = probs[token]\n",
    "                word_probs[tokenizer.decode(token).replace(\" \",\"\")] = word_prob.item()\n",
    "\n",
    "        # Normalización de probabilidades\n",
    "        normalized_word_probs_dummy = {}\n",
    "        total = sum(word_probs.values())\n",
    "        for word in word_probs:\n",
    "            normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "\n",
    "        print(f\"Normalized probabilities for '{' '.join(current_sentence)}': {normalized_word_probs_dummy}\")\n",
    "    \n",
    "    return normalized_word_probs_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "824712c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> The doctor was very famous.She died\n",
      "Normalized probabilities for 'died yesterday in a very sad way due to a heart attack .': {'died': 2.4851276797917072e-05, 'yesterday': 0.009371344762952174, 'in': 0.81797044540868, 'a': 0.1558077807339291, 'very': 0.00985628945473786, 'sad': 3.142454587409808e-05, 'way': 0.0005326831607706446, 'due': 0.002095165633272437, 'to': 0.0036623750956610262, 'heart': 7.568437831769041e-05, 'attack': 1.210678168125465e-06, '.': 0.0005707448708388754}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday\n",
      "Normalized probabilities for 'yesterday in a very sad way due to a heart attack .': {'yesterday': 0.0017902644627721027, 'in': 0.9415960828350984, 'a': 0.00968472630433268, 'very': 0.0026222468114879697, 'sad': 5.60692264089406e-05, 'way': 0.0008705702667654757, 'due': 0.00564310642675988, 'to': 0.009262547430605603, 'heart': 0.00015163835157098337, 'attack': 2.88838684237564e-06, '.': 0.02831985949735556}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in\n",
      "Normalized probabilities for 'in a very sad way due to a heart attack .': {'in': 0.0008046860904732682, 'a': 0.9946914867505543, 'very': 0.0018291170619370069, 'sad': 4.3449458964116255e-05, 'way': 0.00020717826496110503, 'due': 0.0006341402392110553, 'to': 0.00035562547383798007, 'heart': 0.0008454876182166602, 'attack': 5.1490546852691815e-05, '.': 0.0005373384949918546}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a\n",
      "Normalized probabilities for 'a very sad way due to a heart attack .': {'a': 0.016386123396662886, 'very': 0.7009625144793674, 'sad': 0.011637156716849038, 'way': 0.057989787265910454, 'due': 0.0020504945116635955, 'to': 0.002316275531556567, 'heart': 0.1979902920414163, 'attack': 0.004359815171028029, '.': 0.0063075408855457005}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very\n",
      "Normalized probabilities for 'very sad way due to a heart attack .': {'very': 0.30100535372090254, 'sad': 0.6479253096216574, 'way': 0.006737144823270795, 'due': 0.0006850940816759013, 'to': 0.0013543574690672302, 'a': 0.006810997863811491, 'heart': 0.03460705043612884, 'attack': 0.0003945436286414934, '.': 0.00048014835484425767}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad\n",
      "Normalized probabilities for 'sad way due to a heart attack .': {'sad': 0.0022645785346121394, 'way': 0.9893715960576657, 'due': 0.00010614062151113772, 'to': 0.00090867658328478, 'a': 0.0003953483690105654, 'heart': 0.005901224775622578, 'attack': 0.0008067890722185211, '.': 0.00024564598607459536}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way\n",
      "Normalized probabilities for 'way due to a heart attack .': {'way': 0.004715615514776829, 'due': 0.1646936056755702, 'to': 0.20358680924210792, 'a': 0.057067037622846435, 'heart': 0.0007397478089658218, 'attack': 3.750989089093875e-05, '.': 0.5691596742448418}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due\n",
      "Normalized probabilities for 'due to a heart attack .': {'due': 9.230345597053272e-05, 'to': 0.9993508854674616, 'a': 0.0005451825913493996, 'heart': 6.0980674810324915e-06, 'attack': 4.504585613774204e-07, '.': 5.0799591759828025e-06}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due to\n",
      "Normalized probabilities for 'to a heart attack .': {'to': 0.0015727158938608507, 'a': 0.8892394653976408, 'heart': 0.10868326841167185, 'attack': 0.00015826030793492912, '.': 0.0003462899888916584}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due to a\n",
      "Normalized probabilities for 'a heart attack .': {'a': 0.002019704861008219, 'heart': 0.997013298849975, 'attack': 0.0005198599397313409, '.': 0.00044713634928542117}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due to a heart\n",
      "Normalized probabilities for 'heart attack .': {'heart': 0.00014226288408883292, 'attack': 0.99985499837979, '.': 2.7387361212384044e-06}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due to a heart attack\n",
      "Normalized probabilities for 'attack .': {'attack': 0.001723253575282053, '.': 0.998276746424718}\n",
      "<|endoftext|> The doctor was very famous.She died yesterday in a very sad way due to a heart attack .\n",
      "Normalized probabilities for '.': {'.': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Selección aleatoria entre \"He\" y \"She\"\n",
    "symbol_choosen = np.random.choice(list(normalized_word_probs_he_she.keys()), \n",
    "                                  p=list(normalized_word_probs_he_she.values()))\n",
    "normalized_word_probs_dummy = calculate_probabilities_with_prompt_dot(split_lists, 11, tokenizer, model, device, symbol_choosen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
