{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df76bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_prefix_space=True, local_files_only = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=tokenizer_with_prefix_space.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3291b3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[657], [352], [362], [513], [604], [642], [718], [767], [807], [860]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e18a2b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc324540",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \".\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output[0]\n",
    "    probs = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cd5a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "indexes.append([tokenizer.eos_token_id])\n",
    "\n",
    "word_probs = {}\n",
    "for i in indexes:\n",
    "    word_prob = torch.prod(torch.stack([probs[0, -1, idx] for idx in i]))\n",
    "    word_probs[tokenizer.decode(i)] = word_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2775fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 0': 0.03385974360664388,\n",
       " ' 1': 0.03806256264214874,\n",
       " ' 2': 0.018095353245507095,\n",
       " ' 3': 0.018888392716423955,\n",
       " ' 4': 0.022219657806359656,\n",
       " ' 5': 0.026736669754847217,\n",
       " ' 6': 0.01715849815160742,\n",
       " ' 7': 0.022544613293687843,\n",
       " ' 8': 0.014472734370450423,\n",
       " ' 9': 0.015038818295953986,\n",
       " '<|endoftext|>': 0.7729229561163697}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_word_probs = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "298d3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "indexes = [tokenizer.encode(number) for number in numbers]\n",
    "\n",
    "word_probs = {}\n",
    "for i in indexes:\n",
    "    word_prob = torch.prod(torch.stack([probs[0, -1, idx] for idx in i]))\n",
    "    word_probs[tokenizer.decode(i)] = word_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "951c2f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 0': 0.1491112576927676,\n",
       " ' 1': 0.16761959725728418,\n",
       " ' 2': 0.07968816634225867,\n",
       " ' 3': 0.08318054697815978,\n",
       " ' 4': 0.0978507445153572,\n",
       " ' 5': 0.1177427242207227,\n",
       " ' 6': 0.07556245166024182,\n",
       " ' 7': 0.09928178079172653,\n",
       " ' 8': 0.06373490742581289,\n",
       " ' 9': 0.06622782311566863}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_word_probs = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3eb3da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probs(prompt, eos, numbers):\n",
    "    \n",
    "    bos_token_id = [tokenizer.bos_token_id,]\n",
    "\n",
    "    #input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)  \n",
    "    str_prompt = [tokenizer.tokenize(x) for x in prompt]\n",
    "    str_prompt = [item for tokens in str_prompt for item in tokens]\n",
    "    prompt_ids = tokenizer.convert_tokens_to_ids(str_prompt)\n",
    "    \n",
    "    \n",
    "    sequences_id = tokenizer.convert_tokens_to_ids(numbers)\n",
    "    input_ids = torch.tensor(bos_token_id + prompt_ids + sequences_id).reshape(1, -1)\n",
    "    with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            logits = output.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)[0] \n",
    "            \n",
    "    print(input_ids)\n",
    "    input_ids = input_ids[0]\n",
    "    indexes = [tokenizer.encode(number) for number in numbers]\n",
    "    if eos:\n",
    "        input_ids.append([tokenizer.eos_token_id])\n",
    "    word_probs = {}\n",
    "    for i in inputs_ids:\n",
    "\n",
    "        token_probs = []\n",
    "        joint_prob = 1.0\n",
    "        for token_id in i:\n",
    "            token_prob = probs[token_id].item()\n",
    "            token_probs.append(token_prob)  # Guardamos la probabilidad individual del token\n",
    "            joint_prob *= token_prob  # Multiplicamos las probabilidades para la probabilidad conjunta\n",
    "            word = tokenizer.decode(i).replace(\" \", \"\")\n",
    "            word_probs[word] = token_probs    \n",
    "        normalized_word_probs = {}\n",
    "    \n",
    "    for word in word_probs:\n",
    "        normalized_word_probs[word] = word_probs[word] / total\n",
    "    return normalized_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e85d1ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1263/3242330360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnormalized_word_probs_he_she\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1263/2673418492.py\u001b[0m in \u001b[0;36mcalculate_probs\u001b[0;34m(prompt, eos, numbers)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs_ids' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "symbols = [\"He \", \"She \"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]\n",
    "\n",
    "for i in range(12):\n",
    "    next_token = \"\"\n",
    "    prompt = [tokenizer.decode(tokenizer.bos_token_id) , \"The doctor was very famous.\"]\n",
    "    min_digits = 1+ (len(symbols) - 1)\n",
    "    max_digits = len(symbols) + len(symbols_dummy[i].split(\" \"))\n",
    "    count_branching = 0\n",
    "    while next_token != tokenizer.decode(tokenizer.eos_token_id):\n",
    "        if len(prompt) >= min_digits+1:\n",
    "\n",
    "            normalized_word_probs = calculate_probs(prompt, True, symbols_dummy[i].split(\" \"))\n",
    "\n",
    "        else:\n",
    "            normalized_word_probs_he_she = calculate_probs(prompt, False, symbols)\n",
    "        \n",
    "        \n",
    "        if count_branching == 0:\n",
    "            next_token = np.random.choice(a=list(normalized_word_probs_he_she), p=list(normalized_word_probs_he_she.values()))\n",
    "            count_branching = 1\n",
    "        else:\n",
    "            next_token = np.random.choice(a= list(normalized_word_probs), p=list(normalized_word_probs.values()))\n",
    "        \n",
    "        if next_token != tokenizer.decode(tokenizer.eos_token_id):\n",
    "            prompt.append(next_token)\n",
    "        \n",
    "        if len(prompt)-1 >=max_digits:\n",
    "            next_token = tokenizer.decode(tokenizer.eos_token_id)\n",
    "    print(''.join(prompt[1:]))\n",
    "    results.append(''.join(prompt[1:]))\n",
    "    df = pd.DataFrame(results, columns=[\"floating-point\"])\n",
    "    df.to_csv(\"floating_points_LLM.csv\", index=False)\n",
    "    \n",
    "    print(\"///////////////////////////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b5fa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"He\", \"She\"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc6ea874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['died', 'yesterday', 'in', 'a', 'very', 'sad.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_dummy[5].split(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
