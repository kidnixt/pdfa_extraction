{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc7575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_prefix_space=True, local_files_only = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c0792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_important = [\"He\", \"She\"]\n",
    "symbols_dummy = [\"died.\", \n",
    "                 \"died yesterday.\",\n",
    "                 \"died yesterday in.\", \n",
    "                 \"died yesterday in a.\", \n",
    "                 \"died yesterday in a very.\", \n",
    "                 \"died yesterday in a very sad.\", \n",
    "                 \"died yesterday in a very sad way.\", \n",
    "                 \"died yesterday in a very sad way due.\", \n",
    "                 \"died yesterday in a very sad way due to.\", \n",
    "                 \"died yesterday in a very sad way due to a.\",\n",
    "                 \"died yesterday in a very sad way due to a heart.\", \n",
    "                 \"died yesterday in a very sad way due to a heart attack.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ee5b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[679, 1375]\n",
      "['He', 'She']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(symbols_important))\n",
    "print(symbols_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34db437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols: died.\n",
      "Tokens: [3724, 13]\n",
      "Symbols: died yesterday.\n",
      "Tokens: [3724, 7415, 13]\n",
      "Symbols: died yesterday in.\n",
      "Tokens: [3724, 7415, 287, 13]\n",
      "Symbols: died yesterday in a.\n",
      "Tokens: [3724, 7415, 287, 257, 13]\n",
      "Symbols: died yesterday in a very.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 13]\n",
      "Symbols: died yesterday in a very sad.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 13]\n",
      "Symbols: died yesterday in a very sad way.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 13]\n",
      "Symbols: died yesterday in a very sad way due.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 2233, 13]\n",
      "Symbols: died yesterday in a very sad way due to.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 2233, 284, 13]\n",
      "Symbols: died yesterday in a very sad way due to a.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 2233, 284, 257, 13]\n",
      "Symbols: died yesterday in a very sad way due to a heart.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 2233, 284, 257, 2612, 13]\n",
      "Symbols: died yesterday in a very sad way due to a heart attack.\n",
      "Tokens: [3724, 7415, 287, 257, 845, 6507, 835, 2233, 284, 257, 2612, 1368, 13]\n"
     ]
    }
   ],
   "source": [
    "for symbols in symbols_dummy:\n",
    "    tokens = tokenizer.encode(symbols)\n",
    "    print(f\"Symbols: {symbols}\")\n",
    "    print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347f1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   383,  6253,   373,   845,  5863,    13]])\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous.\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output.logits[:, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14d3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'She']\n",
      "[[679], [1375]]\n",
      "[679]\n",
      "[1375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'He': 0.9474213481974543, 'She': 0.0525786518025457}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probs = {}\n",
    "print(symbols_important)\n",
    "symbols_important_ids = [tokenizer.encode(sym) for sym in symbols_important]\n",
    "print(symbols_important_ids)\n",
    "for i in symbols_important_ids :\n",
    "    print(i)\n",
    "    \n",
    "    word_prob = probs[i]\n",
    "    word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "\n",
    "    \n",
    "normalized_word_probs_he_she = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs_he_she[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs_he_she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6fbbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "choices = []\n",
    "num_choices = 1000\n",
    "for _ in range(num_choices):\n",
    "    symbol_choosen = np.random.choice(list(normalized_word_probs_he_she.keys()), \n",
    "                                       p=list(normalized_word_probs_he_she.values()))\n",
    "    choices.append(symbol_choosen)\n",
    "\n",
    "frequency = Counter(choices)\n",
    "\n",
    "plt.bar(frequency.keys(), frequency.values())\n",
    "plt.xlabel('Símbolos Elegidos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de Símbolos Elegidos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d377239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>The doctor was very famous. He\n",
      "<|endoftext|>The doctor was very famous.He\n",
      "tensor([[50256,   383,  6253,   373,   845,  5863,    13,   679]])\n",
      "tensor([[50256,   383,  6253,   373,   845,  5863,    13,  1544]])\n"
     ]
    }
   ],
   "source": [
    "symbol_choosen = np.random.choice(list(normalized_word_probs_he_she.keys()), \n",
    "                                       p=list(normalized_word_probs_he_she.values()))\n",
    "\n",
    "prompt_v2 = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous. \" + symbol_choosen\n",
    "print(prompt_v2)\n",
    "\n",
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous.\" + symbol_choosen\n",
    "print(prompt)\n",
    "input_ids_v2 = torch.tensor(tokenizer.encode(prompt_v2)).reshape(1, -1).to(device)\n",
    "print(input_ids_v2)\n",
    "\n",
    "#we gonna use this one now\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).reshape(1, -1).to(device)\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1ca1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output.logits[:, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45733289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['died.']\n",
      "['died', 'yesterday.']\n",
      "['died', 'yesterday', 'in.']\n",
      "['died', 'yesterday', 'in', 'a.']\n",
      "['died', 'yesterday', 'in', 'a', 'very.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', 'a.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', 'a', 'heart.']\n",
      "['died', 'yesterday', 'in', 'a', 'very', 'sad', 'way', 'due', 'to', 'a', 'heart', 'attack.']\n"
     ]
    }
   ],
   "source": [
    "split_lists = [sentence.split() for sentence in symbols_dummy]\n",
    "\n",
    "for lst in split_lists:\n",
    "    print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb182317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['died.']\n",
      "[[3724], [13]]\n",
      "[3724]\n",
      "[13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'died': 0.9881285521100452, '.': 0.011871447889954789}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probs.clear()\n",
    "\n",
    "print(split_lists[0])\n",
    "sentence = split_lists[0]\n",
    "symbols_dummy_ids = [tokenizer.encode(sym) for sym in sentence]\n",
    "symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "print(symbols_dummy_ids)\n",
    "for i in symbols_dummy_ids :\n",
    "    print(i)\n",
    "    \n",
    "    word_prob = probs[i]\n",
    "    word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "    \n",
    "normalized_word_probs_dummy = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992c99fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>The doctor was very famous.He died.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous.\" + symbol_choosen + \" \" +sentence[0]\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5058aa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['died', 'yesterday.']\n",
      "[[3724], [7415], [13]]\n",
      "[3724]\n",
      "[7415]\n",
      "[13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'died': 0.9880029857200155,\n",
       " 'yesterday': 0.00012707495372092723,\n",
       " '.': 0.011869939326263573}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probs.clear()\n",
    "\n",
    "\n",
    "print(split_lists[1])\n",
    "sentence = split_lists[1]\n",
    "symbols_dummy_ids = [tokenizer.encode(sym) for sym in sentence]\n",
    "symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "print(symbols_dummy_ids)\n",
    "for i in symbols_dummy_ids :\n",
    "    print(i)\n",
    "    \n",
    "    word_prob = probs[i]\n",
    "    word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "    \n",
    "normalized_word_probs_dummy = {}\n",
    "total = sum(word_probs.values())\n",
    "for word in word_probs:\n",
    "    normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "\n",
    "normalized_word_probs_dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01df91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "died\n",
      "['died', 'yesterday.']\n",
      "{'died': 0.9880029857200155, 'yesterday': 0.00012707495372092723, '.': 0.011869939326263573}\n",
      "yesterday.\n",
      "['died', 'yesterday.']\n",
      "{'died': 0.9880029857200155, 'yesterday': 0.00012707495372092723, '.': 0.011869939326263573}\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.decode(tokenizer.bos_token_id) + \"The doctor was very famous.\" + symbol_choosen + \" \" +sentence[0]\n",
    "\n",
    "\n",
    "for words in sentence:\n",
    "    print(words)\n",
    "    symbols_dummy_ids = [tokenizer.encode(sym) for sym in sentence]\n",
    "    symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "    for i in symbols_dummy_ids:\n",
    "        word_prob = probs[i]\n",
    "        word_probs[tokenizer.decode(i).replace(\" \",\"\")] = word_prob.item()\n",
    "        \n",
    "    \n",
    "    normalized_word_probs_dummy.clear()\n",
    "    normalized_word_probs_dummy = {}\n",
    "    total = sum(word_probs.values())\n",
    "    for word in word_probs:\n",
    "        normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "        \n",
    "    print(sentence)\n",
    "    \n",
    "        \n",
    "    print(normalized_word_probs_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1689fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: ['died', 'yesterday.']\n",
      "Current sentence: ['died', 'yesterday.']\n",
      "Token IDs: [[3724], [7415], [13]]\n",
      "Normalized probabilities for 'died yesterday.': {'died': 0.9880029857200155, 'yesterday': 0.00012707495372092723, '.': 0.011869939326263573}\n",
      "Current sentence: ['yesterday.']\n",
      "Token IDs: [[7415], [13]]\n",
      "Normalized probabilities for 'yesterday.': {'yesterday': 0.010592214925753296, '.': 0.9894077850742468}\n"
     ]
    }
   ],
   "source": [
    "def calculate_probabilities(split_lists, index, tokenizer, model, device):\n",
    "    # Tomar la lista de palabras en la posición 'index' de split_lists\n",
    "    sentence = split_lists[index]\n",
    "    print(f\"Original sentence: {sentence}\")\n",
    "\n",
    "    # Iterar sobre la lista reduciendo el tamaño en cada paso\n",
    "    for i in range(len(sentence)):\n",
    "        # Limpiar word_probs para cada sublista\n",
    "        word_probs.clear()\n",
    "\n",
    "        # Usar solo las palabras desde el índice actual hasta el final\n",
    "        current_sentence = sentence[i:]\n",
    "\n",
    "        # Codificar los tokens y organizarlos en la forma [[token_id]]\n",
    "        symbols_dummy_ids = [tokenizer.encode(sym) for sym in current_sentence]\n",
    "        symbols_dummy_ids = [[token_id] for sublist in symbols_dummy_ids for token_id in sublist]\n",
    "\n",
    "        print(f\"Current sentence: {current_sentence}\")\n",
    "        print(f\"Token IDs: {symbols_dummy_ids}\")\n",
    "\n",
    "        # Recalcular las probabilidades para los tokens actuales\n",
    "        for token_ids in symbols_dummy_ids:\n",
    "            for token in token_ids:\n",
    "                word_prob = probs[token]  # Asumiendo que 'probs' ya fue calculado en el contexto del modelo\n",
    "                word_probs[tokenizer.decode(token).replace(\" \",\"\")] = word_prob.item()\n",
    "\n",
    "        # Normalizar las probabilidades\n",
    "        normalized_word_probs_dummy = {}\n",
    "        total = sum(word_probs.values())\n",
    "        for word in word_probs:\n",
    "            normalized_word_probs_dummy[word] = word_probs[word] / total\n",
    "\n",
    "        print(f\"Normalized probabilities for '{' '.join(current_sentence)}': {normalized_word_probs_dummy}\")\n",
    "    \n",
    "    return normalized_word_probs_dummy\n",
    "\n",
    "# Ejemplo de uso:\n",
    "normalized_word_probs_dummy = calculate_probabilities(split_lists, 1, tokenizer, model, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
